#!/usr/bin/env python3
"""
åˆ›å»ºæ··åˆäººæ•°æ•°æ®é›†ç”¨äºæµ‹è¯•åŠ¨æ€äººæ•°åŠŸèƒ½
"""

import numpy as np
import torch
import os
from torch.utils.data import Dataset, DataLoader

class MixedPeopleDataset(Dataset):
    """
    æ··åˆä¸åŒäººæ•°çš„æ•°æ®é›†
    """
    def __init__(self, data_dir, t_his=16, t_pred=14, split='train', train_ratio=0.8):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•
            t_his: è¾“å…¥å¸§æ•°
            t_pred: é¢„æµ‹å¸§æ•°
            split: 'train' æˆ– 'test'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤80%ï¼‰
        """
        self.data_dir = data_dir
        self.t_his = t_his
        self.t_pred = t_pred
        self.split = split
        self.train_ratio = train_ratio
        self.samples = []
        
        # åŠ è½½ä¸åŒäººæ•°çš„æ•°æ®æ–‡ä»¶
        data_files = {
            'mix1_6persons.npy': 6,
            # 'mix2_10persons.npy': 10,  # æ³¨é‡Šæ‰10äººæ•°æ®é›†ï¼Œå‡å°æ˜¾å­˜å ç”¨
            'mupots_120_3persons.npy': 3
        }
        
        print("Loading mixed people datasets...")
        for filename, expected_people in data_files.items():
            filepath = os.path.join(data_dir, filename)
            if os.path.exists(filepath):
                try:
                    data = np.load(filepath)
                    print(f"Loaded {filename}: shape={data.shape}")
                    
                    # å¤„ç†æ•°æ®æ ¼å¼
                    if len(data.shape) == 4:  # [N, P, T, JK]
                        data = data.reshape(data.shape[0], data.shape[1], data.shape[2], -1, 3)
                    elif len(data.shape) == 5:  # [N, P, T, J, K]
                        pass
                    else:
                        print(f"Unexpected data shape for {filename}: {data.shape}")
                        continue
                    
                    # åªå–è¶³å¤Ÿé•¿åº¦çš„åºåˆ—
                    total_frames = self.t_his + self.t_pred
                    if data.shape[2] >= total_frames:
                        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
                        num_sequences = data.shape[0]
                        train_size = int(num_sequences * self.train_ratio)
                        
                        if self.split == 'train':
                            sequence_indices = range(0, train_size)
                        else:  # test
                            sequence_indices = range(train_size, num_sequences)
                        
                        # ä»é€‰å®šçš„åºåˆ—ä¸­æŠ½å–æ ·æœ¬
                        for i in sequence_indices:
                            if data.shape[2] >= total_frames:
                                # å›ºå®šé‡‡æ ·ï¼ˆä½¿ç”¨åºåˆ—ç´¢å¼•ä½œä¸ºç§å­ç¡®ä¿å¯å¤ç°ï¼‰
                                np.random.seed(i)  # å›ºå®šéšæœºç§å­
                                start_idx = np.random.randint(0, max(1, data.shape[2] - total_frames + 1))
                                sample = data[i, :, start_idx:start_idx + total_frames]
                                
                                # ç¡®ä¿äººæ•°æ­£ç¡®
                                actual_people = sample.shape[0]
                                if actual_people > 0:
                                    self.samples.append({
                                        'data': sample,
                                        'people_count': actual_people,
                                        'source': filename,
                                        'seq_idx': i  # æ·»åŠ åºåˆ—ç´¢å¼•ç”¨äºè°ƒè¯•
                                    })
                    
                    print(f"Extracted {len([s for s in self.samples if s['source'] == filename])} samples from {filename}")
                    
                except Exception as e:
                    print(f"Error loading {filename}: {e}")
            else:
                print(f"File not found: {filepath}")
        
        print(f"Total samples: {len(self.samples)}")
        
        # ç»Ÿè®¡äººæ•°åˆ†å¸ƒ
        people_counts = [s['people_count'] for s in self.samples]
        unique, counts = np.unique(people_counts, return_counts=True)
        print(f"People distribution: {dict(zip(unique, counts))}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        data = sample['data']  # [P, T, J, 3]
        
        # ç¡®ä¿æ•°æ®æ ¼å¼: [P, T, J*3]
        if len(data.shape) == 4:  # [P, T, J, 3]
            data = data.reshape(data.shape[0], data.shape[1], -1)
        
        return torch.FloatTensor(data)


def collate_mixed_batch(batch):
    """
    å¤„ç†ä¸åŒäººæ•°çš„æ‰¹æ¬¡ï¼Œæ¨¡æ‹ŸåŸå§‹3DPWæ•°æ®é›†æ ¼å¼
    """
    # æ‰¾åˆ°æœ€å¤§äººæ•°
    max_people = max(sample.shape[0] for sample in batch)
    batch_size = len(batch)
    time_steps = batch[0].shape[1]
    joint_features = batch[0].shape[2]  # 45 = 15 joints * 3 coords
    
    # è®¡ç®—å…³èŠ‚å’Œåæ ‡ç»´åº¦
    coords_dim = 3
    joints_dim = joint_features // coords_dim  # 15 joints
    
    # åˆ›å»º5ç»´å¼ é‡åŒ¹é…åŸå§‹æ ¼å¼ [B, P, T, J, 3]
    padded_batch = torch.zeros(batch_size, max_people, time_steps, joints_dim, coords_dim)
    padding_mask = torch.zeros(batch_size, max_people, dtype=torch.bool)
    
    for i, sample in enumerate(batch):
        actual_people = sample.shape[0]
        # é‡å¡‘æ ·æœ¬æ•°æ®ï¼š[P, T, JK] -> [P, T, J, 3]
        sample_reshaped = sample.reshape(actual_people, time_steps, joints_dim, coords_dim)
        padded_batch[i, :actual_people] = sample_reshaped
        padding_mask[i, :actual_people] = True
    
    # è¿”å›æ ¼å¼åŒ¹é…åŸå§‹collateå‡½æ•°
    # åŸå§‹å‡½æ•°æœŸæœ›ï¼štensor_data, None, padding_mask
    return padded_batch, None, padding_mask


def test_mixed_people_dataset():
    """
    æµ‹è¯•æ··åˆäººæ•°æ•°æ®é›†
    """
    print("=" * 60)
    print("æµ‹è¯•æ··åˆäººæ•°æ•°æ®é›†ï¼ˆå¸¦ train/test åˆ†å‰²ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®é›†
    data_dir = r"C:\Users\31564\Desktop\EMPMP\data"
    dataset_train = MixedPeopleDataset(data_dir, split='train')
    dataset_test = MixedPeopleDataset(data_dir, split='test')
    
    print(f"\nè®­ç»ƒé›†æ ·æœ¬æ•°: {len(dataset_train)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(dataset_test)}")
    
    # æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åºåˆ—æ˜¯å¦æœ‰é‡å 
    train_seqs = set((s['source'], s['seq_idx']) for s in dataset_train.samples)
    test_seqs = set((s['source'], s['seq_idx']) for s in dataset_test.samples)
    overlap = train_seqs & test_seqs
    
    if overlap:
        print(f"\nâš ï¸  è­¦å‘Šï¼šå‘ç° {len(overlap)} ä¸ªé‡å åºåˆ—ï¼")
    else:
        print("\nâœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ— é‡å ")
    
    # ä½¿ç”¨è®­ç»ƒé›†ç»§ç»­æµ‹è¯•
    dataset = dataset_train
    
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼")
        return False
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=8, 
        shuffle=True, 
        collate_fn=collate_mixed_batch
    )
    
    print("\næ£€æŸ¥æ•°æ®åŠ è½½å™¨...")
    
    for i, (batch_data, _, padding_mask) in enumerate(dataloader):
        print(f"\nBatch {i + 1}:")
        print(f"  Batch shape: {batch_data.shape}")
        print(f"  Padding mask shape: {padding_mask.shape}")
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„äººæ•°
        people_counts = padding_mask.sum(dim=1).numpy()
        print(f"  People counts: {people_counts}")
        print(f"  People range: {people_counts.min()} - {people_counts.max()}")
        print(f"  Average people: {people_counts.mean():.1f}")
        
        # åªæ£€æŸ¥å‰3ä¸ªæ‰¹æ¬¡
        if i >= 2:
            break
    
    print("\nâœ… æ··åˆäººæ•°æ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
    return True


if __name__ == "__main__":
    success = test_mixed_people_dataset()
    if success:
        print("\nğŸ‰ æ··åˆäººæ•°æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼ç°åœ¨å¯ä»¥æµ‹è¯•çœŸæ­£çš„åŠ¨æ€äººæ•°äº†")
    else:
        print("\nğŸ’¥ æ··åˆäººæ•°æ•°æ®é›†åˆ›å»ºå¤±è´¥")